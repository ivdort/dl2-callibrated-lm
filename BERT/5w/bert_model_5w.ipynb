{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training BERT Base for Arbitrary 5W Facts from Scratch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from nltk.translate import bleu\n","import matplotlib.pyplot as plt\n","\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertModel, BertTokenizer, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/dl2-5w-dataset/5W_dataset.csv') \n","print(data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset & Model Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["N_EPOCH = 20\n","BATCH_SIZE = 16\n","WEIGHT_DECAY = 0.01\n","LEARNING_RATE = 3e-5\n","MODEL_NAME = 'bert_model'\n","\n","config = BertConfig(\n","    vocab_size=tokenizer.vocab_size,\n","    hidden_size=768,\n","    num_hidden_layers=12,\n","    num_attention_heads=12,\n","    intermediate_size=3072,\n","    hidden_dropout_prob=0.1,\n","    attention_probs_dropout_prob=0.1,\n","    max_position_embeddings=64,\n","    type_vocab_size=1,\n","    initializer_range=0.02\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class BertDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_length=64):\n","        self.padding = 'max_length'\n","        self.data = df['sentences'].to_list()\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.encodings = self.tokenizer(self.data, padding=self.padding, truncation=True, \n","                                        max_length=self.max_length, return_tensors='pt')\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = item['input_ids'].detach().clone()\n","        return item\n","    \n","def get_sentence_embedding(sentence):\n","    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding='max_length', max_length=64).to(device)\n","    with torch.no_grad():\n","        outputs = bert_model(**inputs)\n","    # getting the embeddings of the [CLS] token\n","    cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","    return cls_embedding\n","\n","def model_predict(sentence_list, printer=False, top_k=3):\n","    n_correct = 0\n","    n_correct_top_k = 0\n","    true_labels = []\n","    predictions = []\n","    confidences = []\n","    \n","    eval_tokenized = tokenizer(sentence_list, padding='longest', truncation=True, return_tensors='pt')\n","    \n","    # randomly masking tokens in each sentence\n","    for i in range(eval_tokenized['input_ids'].shape[0]):\n","        n_tokens = sum(eval_tokenized['attention_mask'][i])\n","        random_mask_idx = random.randint(1,n_tokens-2) # eliminating start & end tokens \n","        true_token = eval_tokenized['input_ids'][i, random_mask_idx].item()\n","        true_labels += [true_token]\n","        eval_tokenized['input_ids'][i, random_mask_idx] = tokenizer.mask_token_id\n","\n","    model.eval()\n","    with torch.no_grad():\n","        eval_outputs = model(**eval_tokenized.to(device))\n","    \n","    eval_logits = eval_outputs['logits']\n","    for i in range(len(sentence_list)):\n","        mask_idx = torch.where(eval_tokenized['input_ids'][i]==tokenizer.mask_token_id)[0].item()\n","        eval_probs = torch.topk(F.softmax(eval_logits[i, mask_idx, :],dim=0), top_k).values    \n","        eval_preds = torch.topk(eval_logits[i, mask_idx, :], top_k).indices\n","        eval_preds_tokens = tokenizer.convert_ids_to_tokens(eval_preds)\n","        predictions.append(eval_preds[0].item())\n","        confidences.append(eval_probs[0].item())\n","        if(eval_preds[0].item()==true_labels[i]):\n","            n_correct += 1\n","        if(true_labels[i] in eval_preds.tolist()):\n","            n_correct_top_k += 1\n","        if(printer):\n","            print(f\"[{i}] SENTENCE.........:\", sentence_list[i])\n","            print(f\"[{i}] MASKED TOKEN.....:\", tokenizer.convert_ids_to_tokens(true_labels[i]))\n","            print(f\"[{i}] TOP {top_k} MODEL PREDS:\", eval_preds_tokens)\n","            print(f\"[{i}] CONFIDENCE:......:\", [round(x, 2) for x in eval_probs.tolist()], '\\n')\n","    print('ACCURACY.............:', round(n_correct/len(sentence_list),2))\n","    print(f\"ACCURACY TOP {top_k}.......:\", round(n_correct_top_k/len(sentence_list),2))\n","    return np.array(true_labels), np.array(predictions), np.array(confidences), n_correct/len(sentence_list), round(n_correct_top_k/len(sentence_list),2)\n","\n","def model_predict_auto(sentence_list, printer=False, temperature=1.0):\n","    bleu_scores = []\n","    cos_scores = []\n","    ate_token_id = tokenizer('ate')['input_ids'][1]\n","    period_token_id = tokenizer('.')['input_ids'][1]\n","    mask_token_id = tokenizer.mask_token_id\n","    eval_tokenized = tokenizer(sentence_list, padding='longest', truncation=True, return_tensors='pt')\n","    \n","    for i in range(eval_tokenized['input_ids'].shape[0]):\n","        mask_idx = torch.where(eval_tokenized['input_ids'][i]==ate_token_id)[0].item() + 1 # right after the 'ate' token\n","        auto_sentence = torch.cat((eval_tokenized['input_ids'][i, :mask_idx], torch.tensor([mask_token_id])), dim=0).to(device)\n","        counter = 0\n","        while True:\n","            model.eval()\n","            with torch.no_grad():\n","                auto_outputs = model(auto_sentence.unsqueeze(0))\n","            auto_logits = auto_outputs['logits']\n","            auto_logits = auto_logits[0, mask_idx, :] / temperature\n","            auto_probs = F.softmax(auto_logits, dim=0)\n","            auto_token = torch.multinomial(auto_probs, 1).to(device)\n","            auto_sentence = torch.cat((auto_sentence[:-1], auto_token), dim=0)\n","            if((counter>15) | (auto_token.item()==period_token_id)):\n","                break\n","            auto_sentence = torch.cat((auto_sentence, torch.tensor([mask_token_id]).to(device)), dim=0)\n","            counter += 1\n","            mask_idx += 1\n","        auto_sentence = ''.join(tokenizer.decode(auto_sentence[1:]))\n","        data['temp_bleu_scores'] = data['sentences'].apply(lambda x: bleu([x.lower().split()], auto_sentence.split(), (1,)))\n","        bleu_scores.append(data['temp_bleu_scores'].max())\n","        if(printer):\n","            print(f\"[{i}] GENERATED SENTENCE:\", auto_sentence)\n","            print(f\"[{i}] CLOSEST BLEU......:\", data['sentences'].iloc[data['temp_bleu_scores'].argmax()])\n","            print(f\"[{i}] BLEU SCORE........:\", round(data['temp_bleu_scores'].max(),4))    \n","        auto_sentence_embeddings = get_sentence_embedding(auto_sentence)\n","        data['temp_cos_similarity'] = data['embeddings'].apply(lambda x: cosine_similarity(x, auto_sentence_embeddings)[0][0])\n","        cos_scores.append(data['temp_cos_similarity'].max())\n","        if(printer):\n","            print(f\"[{i}] CLOSEST COSINE....:\", data['sentences'].iloc[data['temp_cos_similarity'].argmax()])\n","            print(f\"[{i}] COSINE SCORE......:\", round(data['temp_cos_similarity'].max(),4), '\\n')\n","        del data['temp_bleu_scores'], data['temp_cos_similarity']\n","    print('AVG MAX BLEU SIM......:', round(sum(bleu_scores)/len(bleu_scores),4))    \n","    print('AVG MAX COSINE SIM....:', round(sum(cos_scores)/len(cos_scores),4))\n","    return sum(bleu_scores)/len(bleu_scores), sum(cos_scores)/len(cos_scores)\n","\n","def calculate_ece(predictions, labels, confidences, n_bins=10):\n","    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n","    bin_lowers = bin_boundaries[:-1]\n","    bin_uppers = bin_boundaries[1:]\n","    ece = 0.0\n","    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n","        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n","        prop_in_bin = np.mean(in_bin)\n","        if prop_in_bin > 0:\n","            accuracy_in_bin = np.mean(predictions[in_bin] == labels[in_bin])\n","            avg_confidence_in_bin = np.mean(confidences[in_bin])\n","            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n","    return ece.round(4)\n","\n","def plot_reliability_diagram(predictions, labels, confidences, n_bins=10):\n","    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n","    bin_lowers = bin_boundaries[:-1]\n","    bin_uppers = bin_boundaries[1:]\n","    accuracies = (predictions == labels).astype(float)\n","    bin_accuracies = []\n","    bin_confidences = []\n","    \n","    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n","        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n","        prop_in_bin = np.mean(in_bin)\n","        if prop_in_bin > 0:\n","            accuracy_in_bin = np.mean(accuracies[in_bin])\n","            avg_confidence_in_bin = np.mean(confidences[in_bin])\n","            bin_accuracies.append(accuracy_in_bin)\n","            bin_confidences.append(avg_confidence_in_bin)\n","\n","    plt.plot(bin_confidences, bin_accuracies, marker='o', linewidth=1, label='Reliability Curve')\n","    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')\n","    plt.xlabel('Confidence')\n","    plt.ylabel('Perceived Frequency')\n","    plt.title('Reliability Diagram for 5W Predictions')\n","    plt.legend()\n","    plt.savefig(f\"{MODEL_NAME}_curve.png\", bbox_inches='tight')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","dataset = BertDataset(data, tokenizer)\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2)\n","data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=data_collator)\n","\n","model = BertForMaskedLM(config=config).to(device) # BERT()\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","print('model n_parameters:', model.num_parameters())"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["val_set = data['sentences'].head(2000).to_list()\n","training_loss = []\n","training_acc = []\n","training_acc_k = []\n","lowest_loss = 999\n","\n","model.train()\n","for epoch in range(N_EPOCH):\n","    total_loss = 0.0\n","    for step, batch in enumerate(tqdm(data_loader)):\n","        model.zero_grad()\n","        outputs = model(**batch.to(device))\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f\"epoch [{epoch+1}/{N_EPOCH}] avg loss: {total_loss/len(data_loader):.4f}\")\n","    training_loss += [total_loss/len(data_loader)]\n","    _, _, _, acc_1, acc_2 = model_predict(val_set, top_k=3)\n","    training_acc += [acc_1]\n","    training_acc_k += [acc_2]\n","torch.save(model.state_dict(), f\"{MODEL_NAME}.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_axis = list(range(1, N_EPOCH+1))\n","plt.figure(figsize=(12, 6))\n","plt.plot(x_axis, training_loss, color='r', label='training loss')\n","plt.plot(x_axis, training_acc, color='b', label='training acc')\n","plt.plot(x_axis, training_acc_k, color='orange', label='training acc top 3')\n","[plt.axvline(x=_x, ls='--', lw=0.5, c='b') for _x in range(2,N_EPOCH,2)]\n","plt.title('BERT 5W - Training Loss / Accuracy')\n","plt.xlabel('epoch')\n","plt.ylabel('loss / accuracy')\n","plt.legend(loc='upper right')\n","plt.savefig(f\"{MODEL_NAME}_loss.png\", bbox_inches='tight')\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":["# Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","config = BertConfig(\n","    vocab_size=tokenizer.vocab_size,\n","    hidden_size=768,\n","    num_hidden_layers=12,\n","    num_attention_heads=12,\n","    intermediate_size=3072,\n","    hidden_dropout_prob=0.1,\n","    attention_probs_dropout_prob=0.1,\n","    max_position_embeddings=64,\n","    type_vocab_size=1,\n","    initializer_range=0.02\n",")\n","\n","model = BertForMaskedLM(config=config).to(device)\n","model.load_state_dict(torch.load('/kaggle/input/dl2-5w-bert/bert_model.pth'))\n","\n","bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n","bert_model.eval()\n","\n","data['embeddings'] = data['sentences'].apply(lambda x: get_sentence_embedding(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["eval_set = data['sentences'].head(2000).to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_predict_auto(eval_set[:200], printer=False, temperature=1.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["true_labels, predictions, confidences, _, _ = model_predict(eval_set, printer=False, top_k=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["calculate_ece(predictions, true_labels, confidences)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_reliability_diagram(predictions, true_labels, confidences)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4972977,"sourceId":8366154,"sourceType":"datasetVersion"},{"datasetId":4974414,"sourceId":8368041,"sourceType":"datasetVersion"},{"datasetId":5076909,"sourceId":8505787,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":4}
