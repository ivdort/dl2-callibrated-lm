<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS",
        useLabelIds: true
      }
    }
  });
</script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# How to alleviate chatbot hallucinations: <br> from the theory in "Calibrated Language Model must Hallucinate"

### M. Feng | D. de Wilde | Ian | Serdar | Lennard

---

In this blog post, we discuss, reproduce, and extend on the findings of the paper titled ["Calibrated Language Model must Hallucinate"](https://arxiv.org/pdf/2311.14648). The main claim of the paper is that pretrained language models (LMs), even under ideal conditions and statistical calibration, are inherently bound to hallucinate to a certain extent.

The purpose of this blog post is threefold: 
1. Help other researchers understand the theory presented in the paper. 
2. Verify the authors' claims by reproducing the results. 
3. Extend on discussion points of the paper.

---

## Language Models Hallucinate
As language models (LMs) grew their popularities, people have also been noticing a large amount of false information generated by them. One common example is non-existent article references. Those mistakes have been described as LMs' "hallucinations", which has caused problems in different fields where LMs are applied, especially in fields such as healthcare, where hallucinated information can be life-threatening. The study by Kalai et al. [1] provided a mathematical analysis of the problem and derived a lower bound on the hallucination rate of LMs, showing that there is a trade-off between calibration accuracy and hallucination rate. This theoretical work presented statistical insights on hallucination and we would like test whether the predictions are true. By doing so, we are seeking for better strategies in avoiding hallucinations when needed.

## <a name="recap">Hallucination</a>

> This section fisrt introduces the phenomenon of hallucination of language models. 

One may wonder, what is hallucination exactly in this context? As defined in the Merriam-Webster (2023) dictionary, it is “a plausible but false or misleading response generated by an artificial intelligence algorithm.” Imagine an AI generates the following statement and attributes it to Albert Einstein: “The future of humanity lies in the harmony between technology and nature.” While this sounds like something Einstein might say, given his thoughtful reflections on science and society, there’s no record of him actually saying these exact words. This would be a clear example of an AI hallucinating a quote—creating a plausible but fictitious statement and attributing it to a famous individual. 

Hallucinations may seem harmless in the the above scenario, but can cause harzards in others. Suppose a user asks an AI for guidance on treating a severe allergic reaction, and the AI mistakenly generates the following response: “Taking a high dose of aspirin immediately is an effective way to mitigate severe allergic reactions.” Giving an medical guidance like this can be not only inccorect but potentially dangerous. 

To better understand the nature of AI hallucination and manage its consequences, a next question one may ask is how often can AI hallucinate. A reasonable and formal approach to answer this question is to find a lower bound to the rate of hallucinations, that is what the authors worked on in their paper.
<!-- The next question might be what caused this hallucination? -->

### The lower bound for hallucination rate
In the above section is explained what a hallucination is. The paper describes a lower bound for the hallucination rate. What is a lower bound? A lower bound is the least value that a parameter or estimator can take. It's a threshold below which values are not considered realistic given the model or constraints. Lower bounds help define the limitations of estimators and assess the accuracy and dependability of statistical estimates. For instance, in the context of confidence intervals, the lower bound marks the lowest value within the interval.

#### Missing facts (missing mass)
The concept of "missing mass" in the paper relates to the probability associated with unobserved outcomes in a sample. When drawing n independent and identically distributed (i.i.d.) samples from an unknown distribution p over a large number of arbitrary factoids (such as the 5W examples and references), there are likely to be some factoids that do not appear in the training data. The missing mass quantifies the probability of encountering these unseen factoids in future samples. In the paper they use the following notations:

$U$: subset of facts that were unobserved in the $n$ training samples

$p(U)$: the fraction of future samples from this fact distribution $p$ that were not observed in the n training samples

#### MonoFacts estimator of missing fact rate
The Good-Turing estimate of the missing mass [2] represents the fraction of samples (or facts, in our case) that appear exactly once in the training data. In our study, we refer to this as the MonoFacts estimator, as showed in equation 1. Equation 2 states that the MonoFacts estimater provides an estimate of the missing mass that is very close to the true missing mass. Specifically, meaning that as the number of samples n increases, the estimation error decreases at a rate proportional to $\sqrt{1 / n}$. This result is significant because it guarantees that the MonoFacts estimator is a reliable method for estimating the missing mass. The bound indicates that the estimate will be very close to the true value, especially as the number of samples grows, and this reliability holds with high probability for any distribution p.


$$
\hat{MF} := \frac{\text{Number of facts appearing exactly once in training data}}{n} \\ \text{(equation 1)}
$$

$|p(U)-\widehat{M F}|=\tilde{O}(\sqrt{1 / n}) \qquad \qquad \text{(equation 2)}$

#### Hallucination rate (lower bound)
In equation 3 the lower bound of the hallucination rate is given. 

$$
\text{Hallucination rate} \geq \hat{MF} - \text{Miscalibration} - \frac{300|\text{Facts}|}{|\text{Possible hallucinations}|} - \frac{7}{\sqrt{n}}
 \\ \text{(equation 3)}$$

First we can see that the hallucination rate is lower bounded by the MonoFact rates. Meanwhile, there are three other terms which can reduce this lower bound. The first is miscalibration rate, which quantifies how calibrated a model is. The less a model is calibrated, the higher the miscalibration rate, the smaller the lower bound. 

The second term is the ratio between the number of facts and possible hallucinations, scaled by 300. Specifically, this refers to the ratio of the number of arbitrary facts to similar pieces of information that are false. If there are more number of facts than possible hallucinations, the lower bound will be decreased. The influence of the term decreases exponentially with the number of samples/facts, because the growth of possible but false information often follows an exponential distribution because each additional piece of information (like an ingredient in a dish or a guest at an event) multiplies the number of plausible narratives or descriptions exponentially. For example, consider a simple model where each fact about a restaurant could vary independently (e.g., location, type of cuisine, ambiance). The combinations of all these varying elements lead to an exponential growth in the number of plausible but untrue descriptions compared to the limited number of truthful combinations.

Finally, the last term introduces a correction based on the sample size n. It makes the lower bound small when the sample size is small. If the sample size is large, the influence of this term is minimal.

From the equation, we can see why a model must hallucinate if they are calibrated: Since the last two terms are very small with large data sets, the lower bound of hallucination rate is mainly determined by the MonoFact estimator and the miscalibration rate. If the model is calibrated, the miscalibration rate will be low and the hallucination rate will be determined by the MonoFact estimator of the missing facts, which is very likely to be above zero.

## <a name="discover"> Calibration</a>

> this section explain what is calibration and it is relevant to our study

Calibration is to make the confidence of prediction being accurate close to the actual prediction accuracy. For example, given 100 predictions, each with confidence of 0:8, we expect that 80 should be correctly classified \cite{Guo2017}. In another word, we want the confidence level of a model matches the true performance of the model.

In the context of LM, we want to make sure the distribution of the training data is aligned with the distribution of language in reality. Instead of calibrating LMs at token level, the paper adapts semantic level calibration, which considers the probability distribution over pieces of information (facts or hallucinations) contained in the text. Specifically, they define a model calibrated if for any probability $z \in[0,1]$, among the pieces of information it generates with probability $\approx z$, such information occurs on average in $\mathrm{a} \approx z$ fraction of naturally occurring language (ideally the distribution from which training data was drawn).

<!-- <table align="center">
  <tr align="center">
      <td><img src="figures/asyrp.png" width=800></td>
  </tr>
  <tr align="left">
    <td colspan=2><b>Figure 3.</b> Asymmetric reverse process (Asyrp) visualization [8].</td>
  </tr>
</table> -->

## <a name="architecture"> Trade-off between Calibration and hallucination </a>

> this section show the main finding about the trade-off between calibration and hallucination and how the authors derive a theoretical lower bound which shows this relationship.

If an LM is calibrated to reflect realistic distributions of language, it will inevitably include representations of less frequent, arbitrary facts since those are present in the full spectrum of language usage in real world. 
Natural language is highly dimensional and variable. Even slight changes in word choice or sentence structure can lead to entirely new meanings and text outputs. A calibrated language model, attuned to a realistic and broad probability distribution, will inherently be able to explore this high-dimensional space more effectively, thereby generating diverse and previously unseen text, and hence, has a high chance of being incorrect. 

The paper's conclusion suggests that while pretraining LMs for good predictive performance may lead to calibration, additional post-training may be necessary to reduce the rate of hallucination, potentially at the cost of perfect calibration.


## <a name="architecture">Testing the theory: systematic and arbitrary facts</a>

> this secion explain our research objective on testing the prediction on the hallucination rate difference between systematic and arbitrary facts

In the paper, the predictions are based on very minialistic setting. Here, we would like to test whether the theory holds in more realistic settings.

Specifically, we would like to see whether the theoretical prediction on differences between hallucination rate on systematic facts and arbitrary facts are true. 

#### arbitrary facts
Factoids are arbitrary pieces of information which are each either true (facts) or false (hallucinations). Arbitrary facts are pieces of information whose truth or falsity cannot be systematically verified using a set of rules or existing knowledge within the training data. They are often specific, contingent, and context-dependent.
For example, a statement like “Alex had lunch with Sam at Cafe Deli on March 15” is arbitrary because, without specific external data confirming this event, there’s no systematic way to verify its truth. The verification of such facts often requires specific, external, and sometimes unavailable information.

These facts are typically unique or rare occurrences and do not follow predictable patterns that can be inferred through general rules or common knowledge. The missing fact rate for arbitrary facts therefore can be relatively high because the vast diversity and specificity of possible arbitrary facts make it unlikely for the training data to cover all or even most of them. 

Because arbitrary facts are not governed by predictable rules and often lack redundancy in the data (i.e., they don’t appear frequently or consistently), language models have fewer clues to learn their true context and relevance. This makes it more likely for models to “hallucinate” or generate arbitrary facts that are plausible but not accurate.

#### systematic facts
Systematic facts are those that can be validated through logical deduction, established rules, or consistent patterns in the data. They are generally based on objective, verifiable data. For example, mathematical truths (e.g., “5 + 7 = 12”) or scientific facts that follow from established principles (e.g., “water boils at 100°C at sea level”) are systematic because their truth can be consistently verified through empirical evidence or logical reasoning.

These facts are predictable and repeatable. They can often be derived or confirmed through analysis or existing knowledge bases without needing external confirmation of each specific instance. 

Because of the systematic nature of the facts, repeating these patterns are more straightforward, leading to a lower missing fact rate and rate of hallucination.


## <a name="reproduction">Experimental setting</a>

> This section explains our setting for the experiments. We trained three different models using different datasets: a math dataset, a 5W dataset, and an abstract-title dataset. Each dataset is designed to test different aspects of fact generation and estimation accuracy. Here, we provide an overview of our datasets, the models we trained, and the evaluation metrics used.

### Datasets
As mentioned we used three different datasets: a math dataset, a 5W dataset, and an abstract-title dataset. For the math dataset we developed a Python script that generates a large set of simple arithmetic equations along with their answers. This dataset includes operations such as addition, subtraction, and multiplication. For every generated equation a random operator was chosen and two integers randomly sampled between 1 and 10. As a result we have a dataset containing 20,000 equations.

In order to construct the 5W dataset as outlined in the referenced paper, we aimed to produce samples in the ‘who-ate-what-when-where-why’ structure. Our initial step involved the creation of distinct word lists for each component of this structure. Specifically, we compiled a list of 40 different names for the ‘who’ segment, 40 diverse meals for the ‘what’ section, 25 temporal expressions for the ‘when’ category, and 10 varied locations for the ‘where’ element. Subsequently, these words were randomly combined to yield a total of 20,000 unique sentences. For the sake of simplifying the model training process and enhancing overall clarity, we ignored the ‘why’ component from our dataset. The source code for the process could be seen in the repository.

For our abstract-title dataset, we used the arXiv dataset provided by Cornell University on Kaggle [3]. This comprehensive dataset contains metadata for scientific papers, including titles, abstracts, authors, and categories from the arXiv repository. We preprocessed the data so it includes entries with the following fields: id, authors, title, and abstract. To ensure the quality and manageability of the dataset, abstracts longer than 200 words were filtered out. The final dataset consists of 20,000 entries, selected to maintain computational feasibility while providing sufficient data for training and evaluation.

### Model
We used BertForMaskedLM model configured with BERT Base parameters. The following table summarizes the BERT model configuration used for each of the three models in our experiment:

<div align="center">

| Configuration                 | Model 1        | Model 2        | Citations        |
|-------------------------------|----------------|----------------|----------------|
| Layers and Heads              | 12 hidden layers, 12 attention heads | 12 hidden layers, 12 attention heads | 12 hidden layers, 12 attention heads |
| Hidden Size                   | 768            | 768            | 768            |
| Intermediate Size             | 3072           | 3072           | 3072           |
| Dropout                       | 0.1 (hidden states and attention) | 0.1 (hidden states and attention) | 0.1 (hidden states and attention) |
| Position Embeddings           | 512            | 512            | 512            |

</div>

#### Preprocessing
For the math dataset we followed very simple preprocessing steps by just applying a pretrained tokenizer (bert-base-uncased).

For training on 5W dataset, the data was fed into the model batches of 16 sentences at a time. We utilized the BERT tokenizer (bert-base-uncased) for tokenization. During the training, each token had a 20% chance of being masked. 

The preprocessing steps for the Abstract-Title model included tokenization and formatting of the text data. We used the BERT tokenizer (bert-base-uncased) to tokenize the text, with a simplification step where periods were removed from the abstracts to clean the data. Each data entry was then concatenated in the format of abstract[SEP]title to create the input for the model.

#### Training Procedure
A DataLoader was used to handle the training data, employing a DataCollatorForLanguageModeling with a masking probability of 0.2 to facilitate masked language modeling. To optimize the training process, we used the ADAMW optimizer, which is well-suited for handling weight decay in conjunction with adaptive learning rates. The models were trained with the following training parameters:

<div align="center">

| Hyperparameter   | Model 1      | Model 2      | Citations |
|------------------|--------------|--------------|--------------|
| Batch Size       | 16           | 16           | 16           |
| Epochs           | 20           | 20           | 71           |
| Learning Rate    | 3e-5         | 3e-5         | 4e-5         |
| Weight Decay     | 0.01         | 0.01         | 0.01         |

</div>

<img src="ss_5w_loss.png" alt="drawing" width="600"/>

### Evaluation Procedure

To evaluate the performance and hallucination of the models, several metrics were used. Accuracy was measured as the proportion of correctly predicted masked tokens, while top-k accuracy evaluated the proportion of true tokens appearing in the top-k predictions. The average loss per epoch during training was also monitored to track the model's learning progress. 

Results on the math dataset were evaluated via a validation set of 2,000 samples from the same distribution as the train data. During training, it is evaluated in the task of masked token prediction. We also calculated the expected calibration error after each epoch of training to see if the model achieves better calibration through further training. To evaluate the hallucination rate of the model, we made it generate equations. (TODO: explain closeness measure)

In the evaluation phase of 5W model, we masked one random token in each sentence of the evaluation set and tasked the model with predicting the masked token. To assess accuracy, the token with the highest logit value produced by the model was considered as the model’s prediction. For accuracy_top_3 score, we selected the three tokens with the highest logits. If the masked token matched the predicted token(s), it was marked as correct. To evaluate the hallucination rate in generative tasks, we provided the model with a random name from the dataset followed by the token of ‘ate’. The model was then expected to generate the rest of the sentence up to the period token. We calculated the BLEU score between the generated sentence and all other sentences in the dataset, reporting the highest scoring match. Additionally, we compared the embeddings of the generated sentences with the embeddings of the other sentences in the dataset and reported the maximum cosine similarity score.To get the embeddings of the sentences, we utilized the pretrained bert-base-uncased model and extracted the embeddings of the [CLS] token.

<p float="left">
  <img src="ss_5w_acc.png" width="400" />
  <img src="ss_5w_similarity.png" width="400" /> 
</p>

For the generated titles, exact match accuracy was used to determine the percentage of titles that exactly matched the true titles. Additionally, the average similarity score was calculated as the cosine similarity between BERT embeddings of the predicted and true titles, providing a measure of the semantic closeness of the generated content to the original.

To evaluate the Abstract-title model's performance, we used a validation set comprising 2,000 entries sampled from the dataset, which allowed for periodic evaluation during training. The model's predictions were evaluated through a masked token prediction task, where random tokens were masked and the model's accuracy in predicting these tokens was measured. Additionally, we tested the model's ability to generate titles in an autoregressive manner, calculating the sentence-level accuracy of these generated titles to evaluate coherence and relevance. To further evaluate the model's performance, we did a calibratedness check using temperature-scaled (0.6) multinomial sampling. The model-generated titles were compared to true titles using exact match accuracy and cosine similarity of their BERT embeddings.

## <a name="bias">Results</a>
> This section explains our results

### 5W Dataset
We utilized 2,000 samples from the training set as an evaluation set. The results indicated the accuracy score of 69% and the accuracy_top_3 score of 77%. These metrics reflect the model's capability to accurately predict the masked tokens and are essential for assessing the extent to which the model hallucinates the masked token. Considering that the model has been exposed to all sentences during the training phase, we expected high accuracy in predicting masked tokens. 
To assess hallucination in generative tasks, we generated 200 sentences using the model. The results yielded the BLEU score of 0.7 and cosine similarity score of 0.97. These metrics suggest that, while the majority parts of the generated sentences closely adhered to the systematic facts found in the dataset, there were still some tokens within the generated sentences that had not been encountered during training—indicative of hallucination by the model.	
In summary, while the model demonstrated proficiency in predicting masked tokens and maintaining coherence with systematic facts, the presence of hallucinated content in generative tasks highlights the inherent challenges in achieving perfect accuracy. These findings underscore the necessity for continuous refinement and validation to mitigate hallucinations and enhance the reliability of generative models.

In our experiments, we aimed to evaluate the performance and hallucination tendencies of our BERT-based model, focusing on the autoregressive generation of titles based on abstracts in the ArXiv metadata dataset. The metrics used for evaluation were the exact match accuracy, average cosine similarity score, and a reliability diagram to check calibration.

### Calibration
We checked the calibration of our model by comparing the cosine similarity between the true title and the predicted title over a validation set of 2500 unseen samples. We employed temperature-scaled multinomial sampling with temperatures between 0.2 and 1.0. Canging the temperature parameter in affects the randomness of the model's predictions. Lower temperatures (e.g., 0.2, 0.4) make the model's output more focused and accurate, increasing exact match accuracy and similarity scores, while higher temperatures (e.g., 0.8, 1.0) make the predictions more diverse and random, reducing these metrics but generating more varied outputs. The average cosine similarity was found by generating sentence embeddings for the true sentence and the predicted sentence using a pretrained BERT model.

<div align="center">

| Temperature | Exact Match Accuracy | Cosine Similarity |
|-------------|----------------------|-------------------|
| 0.2         | 0                    | 0.88              |
| 0.4         | 0                    | 0.88              |
| 0.6         | 0                    | 0.87              |
| 0.8         | 0                    | 0.87              |
| 1.0         | 0                    | 0.86              |

<br><br>

![Reliability Diagram](reliabilitydiagram.png)

</div>

### Sample generations

<div align="center">

| Abstract | Title | Predicted Title | Similarity Score |
|----------|-------|-----------------|------------------|
| We prove a fractional version of the Hardy--Sobolev--Maz'ya inequality for arbitrary domains and $L^p$ norms with $p\geq 2$. This inequality combines the fractional Sobolev and the fractional Hardy inequality into a single inequality, while keeping the sharp constant in the Hardy inequality. | fractional hardy - sobolev - maz'ya inequality for domains | fractional hardy - sobolev - maz'ya inequality for domains with | 0.9902 |
| We present spectroscopic metallicities of individual stars in seven gas-rich dwarf irregular galaxies (dIrrs), and we show that dIrrs obey the same mass-metallicity relation as the dwarf spheroidal (dSph) satellites of both the Milky Way and M31: Z_* ~ M_*^(0.30 +/- 0.02). The uniformity of the relation is in contradiction to previous estimates of metallicity based on photometry. This relationship is roughly continuous with the stellar mass-stellar metallicity relation for galaxies as massive as M_* = 10^12 M_sun. Although the average metallicities of dwarf galaxies depend only on stellar mass, the shapes of their metallicity distributions depend on galaxy type. The metallicity distributions of dIrrs resemble simple, leaky box chemical evolution models, whereas dSphs require an additional parameter, such as gas accretion, to explain the shapes of their metallicity distributions. Furthermore, the metallicity distributions of the more luminous dSphs have sharp, metal-rich cut-offs that are consistent with the sudden truncation of star formation due to ram pressure stripping. | the universal stellar mass - stellar metallicity relation for dwarf galaxies | a small - mass - stellar metallicity relation of dwarf galaxies in | 0.8700 |
| A comparison of the structural, optical and electronic properties of the recently discovered transparent conducting oxide (TCO), nanoporous Ca12Al14O33, with those of the conventional TCO's (such as Sc-doped CdO) indicates that this material belongs conceptually to a new class of transparent conductors. For this class of materials, we formulate criteria for the successful combination of high electrical conductivity with complete transparency in the visible range. Our analysis suggests that this set of requirements can be met for a group of novel materials called electrides. | combining high conductivity with complete optical transparency : a band - structure approach | high electrical conductivity with increased illumination transparency : a band gap prem. we | 0.8900 |
| The global properties of spatially homogeneous cosmological models with collisionless matter are studied. It is shown that as long as the mean curvature of the hypersurfaces of homogeneity remains finite no singularity can occur in finite proper time as measured by observers whose worldlines are orthogonal to these hypersurfaces. Strong cosmic censorship is then proved for the Bianchi I, Bianchi IX and Kantowski-Sachs symmetry classes. | cosmic censorship for some spatially homogeneous cosmological models | cosmic censorship in both spatially homogeneous cosmological models with | 0.9035 |
| In this paper we prove a mirror symmetry conjecture based on the work of Brini-Eynard-Mari\~no \cite{BEM} and Diaconescu-Shende-Vafa \cite{DSV}. This conjecture relates open Gromov-Witten invariants of the conifold transition of a torus knot to the topological recursion on the B-model spectral curve. | topological recursion for the conifold transition of a torus knot | mirror recursion of the conifold transition of the torus. this | 0.8682 |

</div>


### Analysis
Our results reveal an exact match accuracy of 0.0 for all temperatures. This indicates that the model was not able to generate any titles which exactly matched the true titles. The average cosine similarity score was between 0.86 and 0.88 however, suggesting that while the model was not able to generate any exact matches, its generations were semantically still similar to the true titles, as you can also see in the five randomly sampled abstract title pairs in the table above. 

Reliability Curve (see the reliability diagram above):

The reliability curve (blue line) shows the relationship between the model's confidence in its predictions (x-axis) and the actual accuracy of these predictions (y-axis).
The closer this curve is to the diagonal line (orange dashed line), the better calibrated the model is. This means the model's predicted probabilities accurately reflect the true likelihood of events. The diagonal line represents perfect calibration. For a perfectly calibrated model, if it predicts an event with 70% confidence, that event should occur 70% of the time. Deviations from this line indicate miscalibration. Above the line means the model is underconfident, while below means it's overconfident. In our reliability diagram, the reliability curve closely follows the diagonal line, indicating good calibration overall. Slight deviations from the diagonal suggest areas where the model might be slightly underconfident or overconfident. 

We also calculated an Expected Calibration Error (ECE) score according to these 10 bins you see in the diagram. Expected Calibration Error is a metric used to quantify how well the predicted probabilities of a model align with the actual outcomes. It measures the difference between confidence levels (the predicted probabilities) and the actual accuracy of those predictions, aggregated over several confidence intervals or bins. A lower ECE indicates better calibration, meaning the model's confidence in its predictions accurately reflects the true likelihood of those predictions being correct. The ECE score for this model when predicting masked tokens in the title is 0.030424838516116135.



## Further Research: 
> This section discusses our ideas for future work.
To further improve the performance and reliability of our model, several avenues for future work have been identified. Firstly, developing a custom tokenizer tailored to our specific dataset could enhance the tokenization process, potentially increasing the accuracy of the masked token prediction and reducing hallucination in generative tasks. Additionally, leveraging a more comprehensive and powerful dataset could provide the model with a richer context, thereby improving its ability to generate coherent and factually accurate sentences. Exploring different models to assess the hallucination rate would also be beneficial; by comparing results across various architectures, we can identify which models are more effective in minimizing hallucinations. Finally, conducting extended training over more epochs could provide insights into the model's learning curve. Specifically, tracking the loss over additional epochs would help determine whether the model's performance continues to improve or not, thereby informing decisions on optimal training duration. By addressing these areas, we aim to enhance the robustness and accuracy of our model, reducing hallucination and increasing its reliability for real-world applications.

## Concluding Remarks
> This section concludes the insights of this blogpost.

## Authors' Contributions

- Mengli: 
- Ian: 
- Serdar: 
- Lennard:
- Devin: 

## Bibliography

[1] Kalai, A. T., & Vempala, S. S. (2023). Calibrated language models must hallucinate. arXiv preprint arXiv:2311.14648.

[2] I. J. Good. 1953. The Population Frequences of Species and the Estimation of Population Parameters.
Biometrika 40, 3-4 (Dec. 1953), 237–264.

[3] Cornell University. (n.d.). arXiv dataset. Retrieved from https://www.kaggle.com/datasets/Cornell-University/arxiv
