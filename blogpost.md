# How to alleviate chatbot hallucinations: <br> from the theory in "Calibrated Language Model must Hallucinate"

### M. Feng

---

In this blog post, we discuss, reproduce, and extend on the findings of the paper titled ["Calibrated Language Model must Hallucinate"](https://arxiv.org/pdf/2311.14648). The main claim of the paper is that pretrained language models (LMs), even under ideal conditions and statistical calibration, are inherently bound to hallucinate to a certain extent.

The purpose of this blog post is threefold: 
1. Help other researchers understand the theory presented in the paper. 
2. Verify the authors' claims by reproducing the results. 
3. Extend on discussion points of the paper.

---

## Language Models Hallucinate
As language models (LMs) grew their popularities, people have also been noticing a large amount of false information generated by them. One common example is non-existent article references. Those mistakes have been described as LMs' "hallucinations", which has caused problems in different fields where LMs are applied, especially in fields such as healthcare, where hallucinated information can be life-threatening. The study by \cite{Kalai2023} provided a mathematical analysis of the problem and derived a lower bound on the hallucination rate of LMs, showing that there is a trade-off between calibration accuracy and hallucination rate. This theoretical work presented statistical insights on hallucination and we would like test whether the predictions are true. By doing so, we are seeking for better strategies in avoiding hallucinations when needed.

## <a name="recap">Hallucination</a>

> This section fisrt introduces the phenomenon of hallucination of language models. 

One may wonder, what is hallucination exactly in this context? As defined in the Merriam-Webster (2023) dictionary, it is “a plausible but false or misleading response generated by an artificial intelligence algorithm.”

Imagine an AI generates the following statement and attributes it to Albert Einstein: “The future of humanity lies in the harmony between technology and nature.” While this sounds like something Einstein might say, given his thoughtful reflections on science and society, there’s no record of him actually saying these exact words. This would be a clear example of an AI hallucinating a quote—creating a plausible but fictitious statement and attributing it to a famous individual. 

Hallucinations may seem harmless in this scenario, but can cause harzards in others. Suppose a user asks an AI for guidance on treating a severe allergic reaction, and the AI mistakenly generates the following response: “Taking a high dose of aspirin immediately is an effective way to mitigate severe allergic reactions.” Giving an medical guidance like this can be not only inccorect but potentially dangerous. 

To better understand the nature of AI hallucination and manage its consequences, a next question one may ask is how often can AI hallucinate. A reasonable and formal approach to answer this question is to find a lower bound to the rate of hallucinations, that is what the authors worked on in their paper.
<!-- The next question might be what caused this hallucination? -->

### the lower bound for hallucination rate

#### missing facts (missing mass)
$U$: subset of facts that were unobserved in the $n$ training samples

$p(U)$: the fraction of future samples from this fact distribution $p$ that were not observed in the n training samples

#### MonoFacts estimator of missing fact rate
$\widehat{M F}:=\frac{\text { Number of facts appearing exactly once in training data }}{n}$.

$|p(U)-\widehat{M F}|=\tilde{O}(\sqrt{1 / n})$ with high probability for any distribution $p$

This quantifies the facts that rarely appears

#### Hallucination rate (lower bound)
Hallucination rate $\geq \widehat{M F}-$ Miscalibration $-\frac{300 \mid \text { Facts } \mid}{\mid \text { Possible hallucinations } \mid}-\frac{7}{\sqrt{n}}$

$\frac{300 \mid \text { Facts } \mid}{\mid \text { Possible hallucinations } \mid}$

$\frac{7}{\sqrt{n}}$: small for large training set size $n$: the ratio of
the number of arbitrary facts to similar pieces of information that are false, which is exponentially
small for many types of information

#### <a name="discover"> The Lower Bound </a>
Hallucination rate $\geq \widehat{M F}-$ Miscalibration $-\frac{300 \mid \text { Facts } \mid}{\mid \text { Possible hallucinations } \mid}-\frac{7}{\sqrt{n}}$

Notice that hallucinate rate is bounded by missing fact rate.

## <a name="discover"> Calibration</a>

> this section explain what is calibration and it is relevant to our study

Calibration is to make the confidence of prediction being accurate close to the actual prediction accuracy. For example, given 100 predictions, each with confidence of 0:8, we expect that 80 should be correctly classified \cite{Guo2017}. In another word, we want the confidence level of a model matches the true performance of the model.

In the context of LM, we want to make sure the distribution of the training data is aligned with the distribution of language in reality. Instead of calibrating LMs at token level, the paper adapts semantic level calibration, which considers the probability distribution over pieces of information (facts or hallucinations) contained in the text. Specifically, they define a model calibrated if for any probability $z \in[0,1]$, among the pieces of information it generates with probability $\approx z$, such information occurs on average in $\mathrm{a} \approx z$ fraction of naturally occurring language (ideally the distribution from which training data was drawn).

<!-- <table align="center">
  <tr align="center">
      <td><img src="figures/asyrp.png" width=800></td>
  </tr>
  <tr align="left">
    <td colspan=2><b>Figure 3.</b> Asymmetric reverse process (Asyrp) visualization [8].</td>
  </tr>
</table> -->

## <a name="architecture"> Trade-off between Calibration and hallucination </a>

> this section show the main finding about the trade-off between calibration and hallucination and how the authors derive a theoretical lower bound which shows this relationship.

If an LM is calibrated to reflect realistic distributions of language, it will inevitably include representations of less frequent, arbitrary facts since those are present in the full spectrum of language usage in real world. 
Natural language is highly dimensional and variable. Even slight changes in word choice or sentence structure can lead to entirely new meanings and text outputs. A calibrated language model, attuned to a realistic and broad probability distribution, will inherently be able to explore this high-dimensional space more effectively, thereby generating diverse and previously unseen text, and hence, has a high chance of being incorrect. 

The paper's conclusion suggests that while pretraining LMs for good predictive performance may lead to calibration, additional post-training may be necessary to reduce the rate of hallucination, potentially at the cost of perfect calibration.


## <a name="architecture">Testing the theory: systematic and arbitrary facts</a>

> this secion explain our research objective on testing the prediction on the hallucination rate difference between systematic and arbitrary facts

In the paper, the predictions are based on very minialistic setting. Here, we would like to test whether the theory holds in more realistic settings.

Specifically, we would like to see whether the theoretical prediction on differences between hallucination rate on systematic facts and arbitrary facts are true. 

#### arbitrary facts
Factoids are arbitrary pieces of information which are each either true (facts) or false (hallucinations). Arbitrary facts are pieces of information whose truth or falsity cannot be systematically verified using a set of rules or existing knowledge within the training data. They are often specific, contingent, and context-dependent.
For example, a statement like “Alex had lunch with Sam at Cafe Deli on March 15” is arbitrary because, without specific external data confirming this event, there’s no systematic way to verify its truth. The verification of such facts often requires specific, external, and sometimes unavailable information.

These facts are typically unique or rare occurrences and do not follow predictable patterns that can be inferred through general rules or common knowledge. The missing fact rate for arbitrary facts therefore can be relatively high because the vast diversity and specificity of possible arbitrary facts make it unlikely for the training data to cover all or even most of them. 

Because arbitrary facts are not governed by predictable rules and often lack redundancy in the data (i.e., they don’t appear frequently or consistently), language models have fewer clues to learn their true context and relevance. This makes it more likely for models to “hallucinate” or generate arbitrary facts that are plausible but not accurate.

#### systematic facts
Systematic facts are those that can be validated through logical deduction, established rules, or consistent patterns in the data. They are generally based on objective, verifiable data. For example, mathematical truths (e.g., “5 + 7 = 12”) or scientific facts that follow from established principles (e.g., “water boils at 100°C at sea level”) are systematic because their truth can be consistently verified through empirical evidence or logical reasoning.

These facts are predictable and repeatable. They can often be derived or confirmed through analysis or existing knowledge bases without needing external confirmation of each specific instance. 

Because of the systematic nature of the facts, repeating these patterns are more straightforward, leading to a lower missing fact rate and rate of hallucination.


## <a name="reproduction">Experimental setting</a>

> this section explains our setting for the experiments. It includes what language models we use, how we train it, and how the test and measurements are done

### measuring hallucination
The an idealized model in which there are clear-cut facts, where statements that violate these facts would generally be categorized as hallucinations by most definitions

We begin by reproducing the qualitative and quantitative results of the original paper. To sustain the limits of our computational budget, we restrict our efforts to the CelebA-HQ \[6\] dataset. Our experiments are based on the [original implementation](https://github.com/kwonminki/Asyrp_official/tree/main/models), however, we found that some of the features required for successful reproduction, especially those relating to quantitative evaluation, are missing from the repository. Generally, we follow the computational set-up specified by the original authors in full. Specifically, we use hyperparameter values presented in Table 1, which were recovered from \[8, Table 2\] and \[8, Table 3\]. Across all experiments, we use $\lambda\_{\text{recon}} = 3 * \frac{\Delta T}{\Vert \Delta T \Vert}$, i.e., the cosine similarity of the source and target prompts, and $t_{\text{boost}} = 167$ as recommended by the original authors. Unless specified otherwise, we use 40 time steps during both the inversion and generation phase of training and inference. We train the models using 1000 training images over a single epoch.

<table align="center">
  <tr align="center">
      <th align="left">Label</th>
      <th align="left">$y_{ref}$</th>
      <th align="left">$y_{target}$</th>
      <th>$\lambda_{\text{CLIP}}$</th>
      <th>$t_{\text{edit}}$</th>
      <th>Domain</th>
  </tr>
  <tr align="center">
    <td align="left">smiling</td>
    <td align="left">"face"</td>
    <td align="left">"smiling face"</td>
    <td>0.8</td>
    <td>513</td>
    <td>IN</td>
  </tr>
  <tr align="center">
    <td align="left">sad</td>
    <td align="left">"face"</td>
    <td align="left">"sad face"</td>
    <td>0.8</td>
    <td>513</td>
    <td>IN</td>
  </tr>
  <tr align="center">
    <td align="left">angry</td>
    <td align="left">"face"</td>
    <td align="left">"angry face"</td>
    <td>0.8</td>
    <td>512</td>
    <td>IN</td>
  </tr>
  <tr align="center">
    <td align="left">tanned</td>
    <td align="left">"face"</td>
    <td align="left">"tanned face"</td>
    <td>0.8</td>
    <td>512</td>
    <td>IN</td>
  </tr>
  <tr align="center">
    <td align="left">man</td>
    <td align="left">"a person"</td>
    <td align="left">"a man"</td>
    <td>0.8</td>
    <td>513</td>
    <td>IN</td>
  </tr>
  <tr align="center">
    <td align="left">woman</td>
    <td align="left">"a person"</td>
    <td align="left">"a woman"</td>
    <td>0.8</td>
    <td>513</td>
    <td>IN</td>
  </tr>
  <tr align="center">
    <td align="left">young</td>
    <td align="left">"person"</td>
    <td align="left">"young person"</td>
    <td>0.8</td>
    <td>515</td>
    <td>IN</td>
  </tr>
  <tr align="center">
    <td align="left">curly hair</td>
    <td align="left">"person"</td>
    <td align="left">"person with curly hair"</td>
    <td>0.8</td>
    <td>499</td>
    <td>IN</td>
  </tr>
  <tr align="center">
    <td align="left">nicolas</td>
    <td align="left">"Person"</td>
    <td align="left">"Nicolas Cage"</td>
    <td>0.8</td>
    <td>461</td>
    <td>UN</td>
  </tr>
  <tr align="center">
    <td align="left">pixar</td>
    <td align="left">"Human"</td>
    <td align="left">"3D render in the style of Pixar"</td>
    <td>0.8</td>
    <td>446</td>
    <td>UN</td>
  </tr>
  <tr align="center">
    <td align="left">neanderthal</td>
    <td align="left">"Human"</td>
    <td align="left">"Neanderthal"</td>
    <td>1.2</td>
    <td>490</td>
    <td>UN</td>
  </tr>
  <tr align="center">
    <td align="left">modigliani</td>
    <td align="left">"photo"</td>
    <td align="left">"Painting in Modigliani style"</td>
    <td>0.8</td>
    <td>403</td>
    <td>UN</td>
  </tr>
  <tr align="center">
    <td align="left">frida</td>
    <td align="left">"photo"</td>
    <td align="left">"self-portrait by Frida Kahlo"</td>
    <td>0.8</td>
    <td>321</td>
    <td>UN</td>
  </tr>
  <tr align="left">
    <td colspan=7><b>Table 1.</b> Hyperparameter settings of reproducibility experiments. The "domain" column corresponds<br>to the attribute being in-domain (IN) or unseen-domain (UN).</td>
  </tr>
</table>

Figure 7 shows that the results obtained in the original paper and presented in \[8, Figure 4\] can be successfully reproduced and that editing in the h-space results in visually convincing image generation for in-domain attributes (i.e., attributes that can be directly observed in the training data of the frozen diffusion model). Nevertheless, we must stress that the methodology does not necessarily isolate attribute changes and particular edits may also result in other unintended changes. To give an example, edits for the "curly hair" attribute result in severe facial transformations and appear to overlap with the "smiling" attribute (see the second and the third row of Figure 7).

<table align="center">
  <tr align="center">
      <td><img src="figures/reproduction/in_1.0.png" width=800></td>
  </tr>
  <tr align="left">
    <td colspan=2><b>Figure 7.</b> Editing results for in-domain attributes.</td>
  </tr>
</table>

Figures 8 and 9 depict the results of our reproducibility experiments focused on unseen-domain attributes (i.e., attributes that cannot be observed in the training data) originally presented in \[8, Figure 5\]. In Figure 8, we use the full $\Delta h_t$ as done by the authors. In Figure 9, we reduce the editing strength by half. We observe that for unseen-domain attributes, reduction of the editing strength can significantly reduce invasiveness of the method.

<table align="center">
  <tr align="center">
      <td><img src="figures/reproduction/unseen_1.0.png" width=500></td>
  </tr>
  <tr align="left">
    <td colspan=2><b>Figure 8.</b> Editing results for unseen-domain attributes.</td>
  </tr>
</table>

<table align="center">
  <tr align="center">
      <td><img src="figures/reproduction/unseen_0.5.png" width=500></td>
  </tr>
  <tr align="left">
    <td colspan=2><b>Figure 9.</b> Editing results for unseen-domain attributes with reduced editing strength ($0.5 \Delta h_t$).</td>
  </tr>
</table>

To appreciate the performance of Asyrp quantitatively, we reproduce evaluation results originally presented in \[8, Table 4\] and compute the directional CLIP score for the same three in-domain attributes ("smiling", "sad", "tanned") and two unseen-domain attributes ("pixar", "neanderthal") on a set of 100 images from the test set. The original code does not implement either of the evaluation metrics or experiments, meaning we do not know which images were used for the calculations. We choose to take the first 100 images in terms of image IDs. The results are reported in Table 2. Contrary to the original authors, we supply standard deviations showing that the results are quite unstable. More importantly, there are clear differences in the achieved results that we cannot easily explain. Nevertheless, we observe a stable trend of higher scores when lowering the editing strength to half as expected because of the decreased impact on $\Delta I$.

<table align="center">
	<tr align="center">
		<th align="left">Metric</th>
		<th></th>
		<th>Smiling (IN)</th>
		<th>Sad (IN)</th>
		<th>Tanned (IN)</th>
		<th>Pixar (UN)</th>
		<th>Neanderthal (UN)</th>
	</tr>
	<tr align="center">
		<td align="left">Original $S_{dir}$</td>
		<td>$\Delta h_t$</td>
		<td>0.921</td>
		<td>0.964</td>
		<td>0.991</td>
		<td>0.956</td>
		<td>0.805</td>
	</tr>
	<tr align="center">
		<td align="left">Reproduced $S_{dir}$</td>
		<td>$\Delta h_t$</td>
		<td>0.955<br>(0.048)</td>
		<td>0.993<br>(0.037)</td>
		<td>0.933<br>(0.040)</td>
		<td>0.931<br>(0.032)</td>
		<td>0.913<br>(0.035)</td>
	</tr>
	<tr align="center">
		<td align="left">Reproduced $S_{dir}$</td>
		<td>$0.5 \Delta h_t$</td>
		<td>0.969<br>(0.047)</td>
		<td>0.999<br>(0.035)</td>
		<td>0.973<br>(0.036)</td>
		<td>0.942<br>(0.031)</td>
		<td>0.952<br>(0.035)</td>
	</tr>
	<tr align="left">
		<td colspan=7><b>Table 2.</b> Directional CLIP score ($S_{dir} \ \uparrow$) for in-domain (IN) and unseen-domain (UN) attributes. Standard<br>deviations are reported in parentheses.</td>
	</tr>
</table>

We do not implement the segmentation consistency score due to its shortcomings described in the previous section and also the absence of information on choices made by the original authors with respect to its calculation. To make up for it, we compute FID scores which should represent a more meaningful choice in the context of image editing. From the FID scores presented in Table 3, one can observe worsening performance when Asyrp performs editing in directions requiring more substantial changes of the original image. As expected, lowering the editing strength also significantly and consistently reduces the impact in terms of FID. Naturally, the distance between the reconstructed and the edited images is consistently lower than the distance between original and edited images.

<table align="center">
	<tr align="center">
		<th align="left">Metric</th>
		<th></th>
		<th>Smiling (IN)</th>
		<th>Sad (IN)</th>
		<th>Tanned (IN)</th>
		<th>Pixar (UN)</th>
		<th>Neanderthal (UN)</th>
	</tr>
	<tr align="center">
		<td align="left" rowspan=2>$FID(\mathbf{x}_{orig}, \mathbf{x}_{edit})$</td>
		<td>$\Delta h_t$</td>
		<td>89.2</td>
		<td>92.9</td>
		<td>100.5</td>
		<td>125.8</td>
		<td>125.8</td>
	</tr>
	<tr align="center">
		<td>$0.5 \Delta h_t$</td>
		<td>73.7</td>
		<td>70.6</td>
		<td>73.7</td>
		<td>89.3</td>
		<td>74.8</td>
	</tr>
	<tr align="center">
		<td align="left" rowspan=2>$FID(\mathbf{x}_{recon}, \mathbf{x}_{edit})$</td>
		<td>$\Delta h_t$</td>
		<td>68.8</td>
		<td>60.5</td>
		<td>81.7</td>
		<td>96.9</td>
		<td>137.3</td>
	</tr>
	<tr align="center">
		<td>$0.5 \Delta h_t$</td>
		<td>44.4</td>
		<td>43.7</td>
		<td>49.7</td>
		<td>61.0</td>
		<td>71.7</td>
	</tr>
	<tr align="left">
		<td colspan=7><b>Table 3.</b> Frechet Inception Distance ($FID \ \downarrow$) for in-domain (IN) and unseen-domain (UN) attributes.</td>
	</tr>
</table>

In Figure 10, we present our reproduction of \[8, Figure 7\] visualizing the linearity of the learned editing directions. For the "smiling" attribute, it is clearly viable to go in the opposite direction of $\Delta h_t$ and still produce semantically meaningful results. Moreover, the editing effect is shown to be continuous in editing strength.

<table align="center">
  <tr align="center">
      <td><img src="figures/reproduction/linearity.png" width=800></td>
  </tr>
  <tr align="left">
    <td colspan=2><b>Figure 10.</b> Image edits for the "smiling" attribute with editing strength in the range from -1 to 1.</td>
  </tr>
</table>

Figure 11 reproduces the results originally presented in \[8, Figure 17\]. When reconstructing images using a diffusion model with a relatively small number of time steps used for generation, we observe a severe loss in texture resulting in smoothed-out faces with limited details. For training, we used 40 time steps during generation. At inference time, we tried to increase this number to 1,000 and found that it is possible to generate additional texture improving the results at the cost of computation time.

<table align="center">
  <tr align="center">
      <td><img src="figures/reproduction/details.png" width=500></td>
  </tr>
  <tr align="left">
    <td colspan=2><b>Figure 11.</b> Comparison of generated images for the "smiling" attribute with 40 and 1000 time steps during generation.</td>
  </tr>
</table>

## <a name="bias">Results</a>


## Further Research: 
Lastly in this blog post we set out to investigate whether Asyrp can also be applied on top of a latent diffusion model. Since LDMs currently represent the state-of-the-art in image generation \[16\], it is reasonable to find out if modifications in the h-space lead to meaningful attribute edits in the original images. Conveniently DDIM, the algorithm on which Asyrp was build, is also the algorithm behind LDMs. However, the diffusion process runs in the latent space instead of the pixel space. A sperate VQ-VAE  is trained \[19\], where the encoder $\mathcal{E}$ is used to compress the image $x_0$ to a smaller latent vector $z_0$ and the decoder $\mathcal{D}$ is used to reconstruct the image $\hat{x}_0$ from the computed latent vector $\hat{z}_0$. All the remaining steps are as described in the [second](#recap) and [third](#discover) section, but replacing $x$ by $z$. This leads to training a neural network $\epsilon\_\theta \left( z_t, t \right)$ and optimizing it with the loss in Equation 15. Furthermore, steps in the reverse process can be sampled with Equation 16.

$$L_{L D M} := \mathbb{E}\_{ \mathcal{E}(x), \epsilon \sim \mathcal{N}(0, 1), t } \left\[ \left\| \epsilon - \epsilon\_\theta \left( z_t, t \right) \right\|_2^2 \right\] \qquad \qquad \text{(Equation 15)}$$

$$z_{t-1} = \sqrt{\alpha\_{t-1}} \mathbf{P}\_t \left( \epsilon\_\theta \left( z_t, t \mid \Delta h_t \right) \right) + \mathbf{D}\_t \left( \epsilon\_\theta \left( z_t, t \right) \right) + \sigma_t v_t \qquad \qquad \text{(Equation 16)}$$

However, to calculate the directional CLIP loss both the reference and the generated image are needed, but the whole point of LDMs is that you do not calculate those every step. One aproach to still use the Asyrp algorithm could be to retrain CLIP for LDM latents instead of images, but this is beyond our scope. Therefor we investigated another aproach in which the images are computed from the latents by running the decoder $\mathcal{D}$ on $z_t$ at every time-step. Initially we questioned whether this approach would be fruitful as the VQ-VAE is only trained to reconstruct real images and not images perturbed by different levels of noise. In GIF 1 the results can be seen of running $\mathcal{D}$ on $z_t$ of a LDM at every time step. While this is no conclusive result, it does seem to hint that this approach would be feasible. 

<table align="center">
  <tr align="center">
	<td><img src="figures/ablation/ldm_decoded_latent_3.gif"></td>
	<td><img src="figures/ablation/ldm_decoded_latent_14.gif"></td>
	<td><img src="figures/ablation/ldm_decoded_latent_24.gif"></td>
  </tr>
  <tr align="left">
    <td colspan=3><b>GIF 1.</b> Running the VQ-VAE decoder on the latent at every time step.</td>
  </tr>
</table>

That being said this section is called future research for a reason. Sadly, the original code-base was not very modular and this made applying Asyrp to another DM or LDM not feasible within the scope of this project. Therefor eventually we decided to keep this as future research.

## Concluding Remarks
The Asyrp model presented in the original paper and thoroughly explained in the [Discovering Semantic Latent Space](#discover) and [Model Architecture](#architecture) sections, successfully discovers a semantic latent space in the bottleneck of frozen diffusion models which allows high quality image editing. This is supported by our reproduction study, which was conducted on the CelebA-HQ dataset, using the pretrained DDPM model. The figures in the [Reproduction of the Experiments](#reproduction) section highlight the editing abilities of the model for both in- (eg. smiling, sad, angry) and unseen-domain (eg. Pixar, Neanderthal, Frida) attributes. For the quantitative evaluation, we used the directional CLIP score, as this was reported in the original paper and the FID score. Both of the two metrics have shown better results for in-domain editing in the case of the reproduction study, and agree with the original findings that Neanderthal is the hardest editing direction. The best results are for "sad" and "smiling". The discovered semantic latent space has the properties of linearity and consistency across timesteps which are validated by our reproduction experiments.

We explored the limitations of the orginal model and discovered two main problems: first, that it is heavily biased and section [Bias of the Asyrp model](#bias) shows that the model performs much better on editing images of caucasian people than non-caucasian and also that individuals frequently change gender and eye color and second, that the model needs retraining and has a different hyperparamenter configuration for each attribute. 

We further investigated the capabilities of Asyrp by changing its architecture from convolutional layers to a transformer encoder, as it was presented in the [Model Architecture](#architecture) section. We then conducted an ablation study on this new architecture and shown the impact of distinct ways of attending to the bottleneck feature map, different ways of aggregating the temporal encodings, various normalization methods and activation functions. We concluded that our best model
outperforms the original Asyrp by evaluating both qualitatively and quantitatively, as it was shown in the [Ablation study](#ablation) section. We got a better FID score than the orignal model and also, by looking at the figures we clearly observed that our model captures and edits more fine grained features, thus having a stronger impact on the quality of the edited image.

## Authors' Contributions

- Jonathan: Initial setup of codebase & implementation of architecture ablation studies, implementation of transformer architectures, implementation of FID metric, training of ablation models, wrote novel model architecture. Loss plots
- Ana: did ablation study for activation functions and normalization, bias research, model architecture diagram, wrote Image Editing with Diffusion Models (partly), Discovering Semantic Latent Space (training loss part), Model Architecture, Evaluating Diffusion Models and Concluding Remarks.
- Luc: did DM vs LDM research and notebook; wrote header, Image Editing Using Diffusion Models (partly), Recap on Diffusion Models, the Discovering Semantic Latent Space, Bias in Editing Directions (partly), Ablation Study, and Further Research: Latent Diffusion Models. 
- Eric: Structure of the repository, implementations, reproducibility experiment configurations, executions, visualizations, and analysis for "Reproduction of the Experiments", collaboration on the initial implementation of the transformer-based architecture, help as needed with run executions, figures, and tables for other sections.

## Bibliography

[1] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. [ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2108.02938). In: CVF International Conference on Computer Vision (ICCV). 2021, pp. 14347–14356.

[2] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. “Arcface: Additive angular margin loss for deep face recognition”. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019, pp. 4690–4699.

[3] Prafulla Dhariwal and Alexander Nichol. “Diffusion models beat gans on image synthesis”. In: Advances in Neural Information Processing Systems 34 (2021), pp. 8780–8794.

[4] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. “StyleGAN-NADA: CLIP-guided domain adaptation of image generators”. In: ACM Transactions on Graphics (TOG) 41.4 (2022), pp. 1–13.[18]

[5] Jonathan Ho, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 6840–6851.

[6] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. “Progressive growing of gans for improved quality, stability, and variation”. In: arXiv preprint arXiv:1710.10196 (2017).

[7] Gwanghyun Kim and Jong Chul Ye. “Diffusionclip: Text-guided image manipulation using diffusion models”. In: (2021).

[8] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. “Diffusion models already have a semantic latent space”. In: arXiv preprint arXiv:2210.10960 (2022).

[9] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. “More control for free! image synthesis with semantic diffusion guidance”. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023, pp. 289–299.

[10] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. “Sdedit: Image synthesis and editing with stochastic differential equations”. In: arXiv preprint arXiv:2108.01073 (2021).

[11] Alexander Quinn Nichol and Prafulla Dhariwal. “Improved denoising diffusion probabilistic models”. In: International Conference on Machine Learning. PMLR. 2021, pp. 8162–8171.

[12] Yong-Hyun Park, Mingi Kwon, Junghyo Jo, and Youngjung Uh. “Unsupervised Discovery of Semantic Latent Directions in Diffusion Models”. In: arXiv preprint arXiv:2302.12469 (2023).

[13] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. “Styleclip: Text-driven manipulation of stylegan imagery”. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021, pp. 2085–2094.

[14] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. “Diffusion autoencoders: Toward a meaningful and decodable representation”. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 10619-10629.

[15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. “Learning transferable visual models from natural language supervision”. In: International conference on machine learning. PMLR. 2021, pp. 8748–8763.

[16] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ̈orn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. 2021. arXiv: 2112.10752 [cs.CV].

[17] Jiaming Song, Chenlin Meng, and Stefano Ermon. “Denoising diffusion implicit models”. In: arXiv preprint arXiv:2010.02502 (2020).

[18] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens and Z. Wojna. [Rethinking the Inception Architecture for Computer Vision](https://ieeexplore.ieee.org/document/7780677). 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 2818-2826, doi: 10.1109/CVPR.2016.308.

[19] Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu. "Neural Discrete Representation Learning". Advances in neural information processing systems 30 (2017). 
