# How to alleviate chatbot hallucinations: <br> from the theory in "Calibrated Language Model must Hallucinate"

### M. Feng

---

In this blog post, we discuss, reproduce, and extend on the findings of the paper titled ["Calibrated Language Model must Hallucinate"](https://arxiv.org/pdf/2311.14648). The main claim of the paper is that pretrained language models (LMs), even under ideal conditions and statistical calibration, are inherently bound to hallucinate to a certain extent.

The purpose of this blog post is threefold: 
1. Help other researchers understand the theory presented in the paper. 
2. Verify the authors' claims by reproducing the results. 
3. Extend on discussion points of the paper.

---

## Language Models Hallucinate
As language models (LMs) grew their popularities, people have also been noticing a large amount of false information generated by them. One common example is non-existent article references. Those mistakes have been described as LMs' "hallucinations", which has caused problems in different fields where LMs are applied, especially in fields such as healthcare, where hallucinated information can be life-threatening. The study by \cite{Kalai2023} provided a mathematical analysis of the problem and derived a lower bound on the hallucination rate of LMs, showing that there is a trade-off between calibration accuracy and hallucination rate. This theoretical work presented statistical insights on hallucination and we would like test whether the predictions are true. By doing so, we are seeking for better strategies in avoiding hallucinations when needed.

## <a name="recap">Hallucination</a>

> This section fisrt introduces the phenomenon of hallucination of language models. 

One may wonder, what is hallucination exactly in this context? As defined in the Merriam-Webster (2023) dictionary, it is “a plausible but false or misleading response generated by an artificial intelligence algorithm.”

Imagine an AI generates the following statement and attributes it to Albert Einstein: “The future of humanity lies in the harmony between technology and nature.” While this sounds like something Einstein might say, given his thoughtful reflections on science and society, there’s no record of him actually saying these exact words. This would be a clear example of an AI hallucinating a quote—creating a plausible but fictitious statement and attributing it to a famous individual. 

Hallucinations may seem harmless in this scenario, but can cause harzards in others. Suppose a user asks an AI for guidance on treating a severe allergic reaction, and the AI mistakenly generates the following response: “Taking a high dose of aspirin immediately is an effective way to mitigate severe allergic reactions.” Giving an medical guidance like this can be not only inccorect but potentially dangerous. 

To better understand the nature of AI hallucination and manage its consequences, a next question one may ask is how often can AI hallucinate. A reasonable and formal approach to answer this question is to find a lower bound to the rate of hallucinations, that is what the authors worked on in their paper.
<!-- The next question might be what caused this hallucination? -->

### the lower bound for hallucination rate

#### missing facts (missing mass)
$U$: subset of facts that were unobserved in the $n$ training samples

$p(U)$: the fraction of future samples from this fact distribution $p$ that were not observed in the n training samples

#### MonoFacts estimator of missing fact rate
$\widehat{M F}:=\frac{\text { Number of facts appearing exactly once in training data }}{n}$.

$|p(U)-\widehat{M F}|=\tilde{O}(\sqrt{1 / n})$ with high probability for any distribution $p$

This quantifies the facts that rarely appears

#### Hallucination rate (lower bound)
Hallucination rate $\geq \widehat{M F}-$ Miscalibration $-\frac{300 \mid \text { Facts } \mid}{\mid \text { Possible hallucinations } \mid}-\frac{7}{\sqrt{n}}$

$\frac{300 \mid \text { Facts } \mid}{\mid \text { Possible hallucinations } \mid}$

$\frac{7}{\sqrt{n}}$: small for large training set size $n$: the ratio of
the number of arbitrary facts to similar pieces of information that are false, which is exponentially
small for many types of information

#### <a name="discover"> The Lower Bound </a>
Hallucination rate $\geq \widehat{M F}-$ Miscalibration $-\frac{300 \mid \text { Facts } \mid}{\mid \text { Possible hallucinations } \mid}-\frac{7}{\sqrt{n}}$

Notice that hallucinate rate is bounded by missing fact rate.

## <a name="discover"> Calibration</a>

> this section explain what is calibration and it is relevant to our study

Calibration is to make the confidence of prediction being accurate close to the actual prediction accuracy. For example, given 100 predictions, each with confidence of 0:8, we expect that 80 should be correctly classified \cite{Guo2017}. In another word, we want the confidence level of a model matches the true performance of the model.

In the context of LM, we want to make sure the distribution of the training data is aligned with the distribution of language in reality. Instead of calibrating LMs at token level, the paper adapts semantic level calibration, which considers the probability distribution over pieces of information (facts or hallucinations) contained in the text. Specifically, they define a model calibrated if for any probability $z \in[0,1]$, among the pieces of information it generates with probability $\approx z$, such information occurs on average in $\mathrm{a} \approx z$ fraction of naturally occurring language (ideally the distribution from which training data was drawn).

<!-- <table align="center">
  <tr align="center">
      <td><img src="figures/asyrp.png" width=800></td>
  </tr>
  <tr align="left">
    <td colspan=2><b>Figure 3.</b> Asymmetric reverse process (Asyrp) visualization [8].</td>
  </tr>
</table> -->

## <a name="architecture"> Trade-off between Calibration and hallucination </a>

> this section show the main finding about the trade-off between calibration and hallucination and how the authors derive a theoretical lower bound which shows this relationship.

If an LM is calibrated to reflect realistic distributions of language, it will inevitably include representations of less frequent, arbitrary facts since those are present in the full spectrum of language usage in real world. 
Natural language is highly dimensional and variable. Even slight changes in word choice or sentence structure can lead to entirely new meanings and text outputs. A calibrated language model, attuned to a realistic and broad probability distribution, will inherently be able to explore this high-dimensional space more effectively, thereby generating diverse and previously unseen text, and hence, has a high chance of being incorrect. 

The paper's conclusion suggests that while pretraining LMs for good predictive performance may lead to calibration, additional post-training may be necessary to reduce the rate of hallucination, potentially at the cost of perfect calibration.


## <a name="architecture">Testing the theory: systematic and arbitrary facts</a>

> this secion explain our research objective on testing the prediction on the hallucination rate difference between systematic and arbitrary facts

In the paper, the predictions are based on very minialistic setting. Here, we would like to test whether the theory holds in more realistic settings.

Specifically, we would like to see whether the theoretical prediction on differences between hallucination rate on systematic facts and arbitrary facts are true. 

#### arbitrary facts
Factoids are arbitrary pieces of information which are each either true (facts) or false (hallucinations). Arbitrary facts are pieces of information whose truth or falsity cannot be systematically verified using a set of rules or existing knowledge within the training data. They are often specific, contingent, and context-dependent.
For example, a statement like “Alex had lunch with Sam at Cafe Deli on March 15” is arbitrary because, without specific external data confirming this event, there’s no systematic way to verify its truth. The verification of such facts often requires specific, external, and sometimes unavailable information.

These facts are typically unique or rare occurrences and do not follow predictable patterns that can be inferred through general rules or common knowledge. The missing fact rate for arbitrary facts therefore can be relatively high because the vast diversity and specificity of possible arbitrary facts make it unlikely for the training data to cover all or even most of them. 

Because arbitrary facts are not governed by predictable rules and often lack redundancy in the data (i.e., they don’t appear frequently or consistently), language models have fewer clues to learn their true context and relevance. This makes it more likely for models to “hallucinate” or generate arbitrary facts that are plausible but not accurate.

#### systematic facts
Systematic facts are those that can be validated through logical deduction, established rules, or consistent patterns in the data. They are generally based on objective, verifiable data. For example, mathematical truths (e.g., “5 + 7 = 12”) or scientific facts that follow from established principles (e.g., “water boils at 100°C at sea level”) are systematic because their truth can be consistently verified through empirical evidence or logical reasoning.

These facts are predictable and repeatable. They can often be derived or confirmed through analysis or existing knowledge bases without needing external confirmation of each specific instance. 

Because of the systematic nature of the facts, repeating these patterns are more straightforward, leading to a lower missing fact rate and rate of hallucination.


## <a name="reproduction">Experimental setting</a>

> This section explains our setting for the experiments. It includes what language models we use, how we train it, and how the test and measurements are done

### measuring hallucination
The an idealized model in which there are clear-cut facts, where statements that violate these facts would generally be categorized as hallucinations by most definitions

## Abstract-Title model 

### Dataset Preparation
The dataset for this experiment is derived from the ArXiv metadata snapshot. We preprocessed the data so it includes entries with the following fields: id, authors, title, and abstract. To ensure the quality and manageability of the dataset, abstracts longer than 200 words were filtered out. The final dataset consists of 20,000 entries, selected to maintain computational feasibility while providing sufficient data for training and evaluation.

### Preprocessing
Preprocessing steps included:

Tokenization: The BERT tokenizer (bert-base-uncased) was used to tokenize the text. To simplify the tokenization process, periods were removed from the abstracts.
Formatting: Each data entry was concatenated in the form of abstract[SEP]title to create the input for the model.
Model Configuration
The BERT model configuration for this experiment is based on the BERT Base architecture, characterized by:

Layers and Heads: 12 hidden layers and 12 attention heads.
Hidden and Intermediate Sizes: A hidden size of 768 and an intermediate size of 3072.
Dropout: Dropout probabilities of 0.1 for both hidden states and attention.
Position Embeddings: A maximum of 512 positional embeddings.

### Training Procedure
The model was trained over 20 epochs, with the following training parameters:

Batch Size: 16
Learning Rate: 3e-5
Weight Decay: 0.01
A DataLoader was used to handle the training data, employing a DataCollatorForLanguageModeling with a masking probability of 0.2 to facilitate masked language modeling. The AdamW optimizer was chosen for its efficiency in handling the training dynamics of transformer models.

### Evaluation Metrics
To assess the performance and hallucination tendencies of the model, several metrics were employed:

Accuracy: The proportion of correctly predicted masked tokens.
Top-k Accuracy: The proportion of true tokens appearing in the top-k predictions.
Loss: The average loss per epoch during training.
Exact Match Accuracy: The percentage of generated titles that exactly match the true titles.
Average Similarity Score: The cosine similarity between BERT embeddings of the predicted and true titles.

### Evaluation Procedure
Validation Set: A validation set of 2000 entries was sampled from the dataset for periodic evaluation during training.
Prediction Task: The model's predictions were evaluated using a masked token prediction task, where random tokens were masked, and the model's accuracy in predicting these tokens was measured.
Autoregressive Title Generation: We let the model generate titles in an autoregressive manner. The sentence-level accuracy of these generated titles were calculated to measure coherence and relevance.
Calibratedness Check: Using temperature scaled (0.6) multinomial sampling, the model generated titles, which were then compared to true titles using exact match accuracy and cosine similarity of their BERT embeddings.


## <a name="bias">Results</a>
> This section explains our results

In our experiments, we aimed to evaluate the performance and hallucination tendencies of our BERT-based model, focusing on the autoregressive generation of titles based on abstracts in the ArXiv metadata dataset. The metrics used for evaluation were the exact match accuracy and average cosine similarity score.

### Calibration
We checked the calibration of our model by comparing the cosine similarity between the true title and the predicted title over two sets of 1000 samples each. The first set contained abstract-title pairs which were also used for training, while the second set contained unseen pairs. We employed temperature-scaled multinomial sampling with a temperature of 0.6, which means the model samples higher probability tokens more often. The average cosine similarity was found by generating sentence embeddings for the true sentence and the predicted sentence using a pretrained BERT model.


|                           | Exact match accuracy | Average Cosine Similarity |
|---------------------------|----------------------|---------------------------|
| Training Data Samples     | 0.0                  | 0.82                      |
| Non-Training Data Samples | 0.0                  | 0.82                      |


### Analysis
Our preliminary results reveal an exact match accuracy of 0.0 for both evaluation sets. This indicates that the model was not able to generate any titles which exactly matched the true titles. The average cosine similarity score was 0.82 however, suggesting that while the model was not able to generate any exact matches, its generations were semantically still relatively similar to the true titles. 

The consistent performance across both training and non-training data samples demonstrates that the model's ability to generate semantically similar titles does not degrade when dealing with unseen data. The high average similarity score indicates that the model has learned to generate titles that are contextually relevant and coherent, despite the lack of exact matches.


## Further Research: 
> This section discusses our ideas for future work.

## Concluding Remarks
> This section concludes the insights of this blogpost.

## Authors' Contributions

- Mengli: 
- Ian: 
- Serdar: 
- Lennard:
- Devin: 

## Bibliography

[1] Kalai, A. T., & Vempala, S. S. (2023). Calibrated language models must hallucinate. arXiv preprint arXiv:2311.14648.

